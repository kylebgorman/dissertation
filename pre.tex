In a justly-famous study, \citet{Pierrehumbert1994} notes the general absense of velar-labial consonant clusters in English (though note the existence of a few exceptions like \emph{pigment} and \emph{rugby}). When I first encountered this study, as an undergraduate, I simply could not integrate this arbitrary gap with a view of the language faculty as the inexhaustable fount of productivity responsible for such wonders as [wʌgz] and \emph{Colorless green ideas sleep furiously}. This shocking observation was filed away for further meditation, in the hope that one day I might grok.

The atoms of linguistic theory tend to be sparsely distributed, with ``few giants'' (/t/, \emph{the}) and ``many dwarves'' (/ʒ/, \emph{zymurgy}) in any sample. Sparsity is exacerbated for complex objects: however common \emph{the} is, any two word sequence containing it will be far less frequent. This dissertation began in earnest with a suggestion from Charles Yang that perhaps sparsity---the missing and rare data which have long plagued statistical approaches to language learning and processing---might be the only reason, e.g., English has no words containing a [k.p] cluster. 

Linguists-in-training are inculcated, early and often, with an respect for productivity---the ``infinite use of finite means'' \citep[8]{ASPECTS}---over all else. Psycholinguists have been more free to pursue less combinatorial, less granular approaches to language. Many developmentalists have argued that the impoverished nature of young children's speech reflects whole phrases stored in memory, not phrase structure rules. It is therefore informative that the relatively sparse distribution of phrases in child speech is a practical \emph{necessity} of generativity and sparsity \citep{Yang2011b}. Natural language engineers, unburdened by the need to construct models which conform to the cognitive constraints imposed by human minds, have endowed statistical parsers with phrasal storage mechanisms ``bilexical'' rules \citep[e.g.,][]{Collins1999}, but these offer little no additional coverage \citep{Bikel2004}. I suspect this is not because stored phrases are inherently uninformative, but rather because only a tin small number of two-word sequences can be reasonably expected to occur with sufficient frequency in any corpus. 

Similarly, theorists of lexical processing postulate that complex words are stored in memory to account for the facilitatory effect of whole word frequency on reaction times to complex words \citep[e.g.,][]{Alegre1999,Baayen1997a,Sereno1997}. But this effect of word frequency obtains regardless of whether or not whole word storage is postulated \citep{Taft2004b,LignosInPress}. Furthermore, in languages with rich agglutinative morphology, the role played by a whole word storage mechanism, should one be evidenced, would necessarily be quite marginal \citep{Anderson1988a,Hankamer1992,Chan2008}. According to one estimate, children acquiring American English hear 165,000 word tokens a week \citep{Swingley2007}. At that rate, it would take more than two months for a child to encounter the 1.5 milllion inflectional variants of a single verb in Archi \citep[467]{Kibrik1998},
%, cited in \citealt[122]{Bauer2001}), 
even if adult speech consisted of nothing but orderly recitation of the inflectional forms of this one verb, one after the other, without repetition.

By
some



I can only hope that this humble theory has even a fraction of the influence on phonotactic thinking that 


There are two ways to read this thesis. First, it can be regarded as evidence for a particular, falsifiable theory of phonotactic learning and knowledge. If, for instance, some future study finds that English speakers rate nonce words containing the unattested [k.p] cluster to be less well-formed than similar words containing [k.t], a cluster found in dozens of English words, then this theory will need to be seriously revised. Suggestive evidence comes an auditory wordlikeness task performed by \citet[782]{Munson2001} which includes this contrast. Speakers rated [bɪk.pɘt] and [fæk.pɘt] slightly more English-like than [bɪk.tɘp] and [fæk.tɘp], respectively, and drew no contrast between the pair [nɪk.pɘt]-[nɪk.tɘp]. 

Other claims are not essential to the theory here, but are rather principled null hypotheses, and evidence to the contrary would result only small modifications to the theory. For instance, it is not claimed that wordlikeness judgements are inherently binary, but rather that the evidence currently available is insufficient to justify the considerable additional complexities of gradient wellformedness models.

Under another reading, the claims here represent a \emph{negative heuristic} guiding the phonologist away from static phonotactics as an explanatory device. Then this work is a claim about something grammars do not contain, not a theory of what they do, much like \emph{Richness of the Base} (RotB), an important intellectual precursor to this work (see \citealt[6]{Bye2001} and \citealt[73]{PE} for further discussion). \emph{Lexicon optimization}, however, is an example of a theory which produces grammars which conform to RotB. 

A strict application of this negative heuristic produces some interesting predictions. If speakers are shown to have internalize a phonotactic generalization, then there must be a phonological process to blame. One clear case where processes, and not merely restrictions on underlying representations, 
is where co-occurrence restrictions cause difficulties in producing nonce words violating those restrictions
\citep{Rose2007,GallagherInPress}

are implicated is in Quecha, where ejective co-occurrence restrictions are reflected in speakers' difficulties in producing nonce words containing multiple ejectives \citep{GallagherInPress}. 

This negative heuristic also allows for the possibility of using psycholinguistic evidence to decide between competing phonological analyses. For instance, there is a long-standing debate concerning the representation of Sanskrit aspiration alternations like those represented by \emph{bodhati}-\emph{bhotsyati} `he wakes-he will wake'. One analysis, going back to the ancient grammarian Pāṇini \citep{Sag1974} and to \citet[\S141f.]{Whitney1889}, posits an underlying representation /bud\asp{}-/ with aspiration shifting leftward in certain contexts: modern proponents of this analysis include \citet{Borowsky1983}, \citet[][59f.]{Hoenigswald1965}, \citet{Kaye1985}, \citet{Sag1974,Sag1976} \citet{Schindler1976}, \citet{Stemberger1980}, and \citet[][\S141f.]{Whitney1889}. An alternative analysis posits an underlying representation /b\asp{}ud\asp{}-/ and an apsirate dissimilation process (\citealt{Anderson1970}, \citealt{Hoard1975}, \citealt[\S3.2]{Kiparsky1965}, \citealt{Phelps1973}, \citealt{Phelps1975b}, \citealt[109f.]{Zwicky1965}). It is not my intention to adjudicate between these competing analyses, or to evaluate the claim that both accounts are deeply flawed \citep{Janda1989}, but simply to note that they make different predictions under the theory proposed here. While the former, aspirate shift analysis denies the existence of diaspirate roots, this fact lacks synchronic motivation, so the co-occurrence restriction go unlearned. On the other hand, under the aspirate dissimilation account, underlying diaspirate roots exist but speakers internalize synchronic processes responsible for their absence on the surface. 

%\citet[38f.]{MacEachern1999} discusses a similar restriction on co-occurign aspirate roots in Ofo, but also notes, following \citet{DeReuse1981}, that this generalization holds both within roots and also within complex words. For instance, the aspiration of ['osk\asp{}a] `crane' disappears when combined with [a'f\asp{}ã] `white': [oskaf\asp{}ã] `white or American egret'. Under the negative heuristic, the root co-occurrence restriction and this alternation are both the result of a phonologically general process of aspirate dissimilation.
