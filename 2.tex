\chapter{Categorical and gradient aspects of wordlikeness judgements} 
\label{gradience}

Two nonce words, \emph{blick} [blɪk] and \emph{bnick} [bnɪk], underlie the claim that speakers can rapidly distinguish between possible and impossible words \citet{Halle1962}. In \emph{SPE}, \citet{SPE} use a third word, \emph{bnzk}, to argue that nonce words fall on a cline of well-formedness. Neither \emph{bnick} nor \emph{bnzk} are possible words of English, they are possible words in other languages: stop-nasal onsets are found in Moroccan Arabic (e.g., \emph{bniqa} `closet') and stop-nasal-fricative-stop words in Imdlawn Tashlhiyt Berber \citep{Dell1985}. However, there is some sense in which \emph{bnzk} is even less English-like than \emph{bnick}. Of this, \citeauthor{SPE} write:

\begin{quote}
Hence, a real solution to the problem of ``admissibility'' will not simply define a tripartite categorization of occurring, accidental gap, and inadmissible, but will define the `degree of admissibility' of each potential lexical matrix in such a way as to\ldots{}make numerous other distinctions of this sort (\emph{SPE}:416--417)
\end{quote}

\noindent
The position that the well-formedness of nonce words is consistent the view of syntactic grammaticality taken in \emph{LSLT} and \emph{Aspects} \citep{LSLT,ASPECTS}, where it is claimed that different syntactic vioaltions result in different degrees of ungrammaticality.

Most recent discussions of the ``problem of admissibility'' focus on this cline of wellformedness as it is evidenced in wordlikeness tasks.

\begin{quote}
When native speakers are asked to judge made-up (nonce) words, their intuitions are rarely all-or-nothing. In the usual case, novel items fall along a gradient cline of acceptability. \citep[][9]{Albright2009a}

In the particular domain of phonotactics gradient intuitions are pervasive: they have been found in every experiment that allowed participants to rate forms on a scale.
\citep[][382]{Hayes2008a}

\ldots{}when judgements are elicited in a controlled fashion from speakers, they always emerge as gradient, including all intermediate values. \citep[371]{Shademan2006} 
\end{quote}

At first blush, this would seem to argue against a naïve account of wellformedness judgements in which ill-formedness results when prosodic parsing fails (e.g., \citealt{Ito1989a}, \citealt{Noske1992}, \citealt{OT}). Consider the possibility that a nonce word is ill-formed if it cannot be syllabified without modification. There is a long history for the proposal that impossible words are those whose surface form cannot be parsed into prosodic structures like syllables without further phonological modification (e.g., \citealt[10f.]{Hooper1973}, \citealt[57f.]{Kahn1976}). There is no doubt that prosodic context is involved in determining where segments may occur. For instance, [ŋ] is a possible coda in English, but not a possible onset. Similarly, the [bn] of \emph{bnick} is judged to be admissible (though unattested) in medial position in English (e.g., \emph{sta}[b.n]\emph{ick}; \citealt[97]{Hooper1973}) but not as an onset. English is unable to sylalbify initial onset /bn/ without further modification, so one might expect underlying /bnɪk/ to be subject to deletion \citep[19f.]{Wolf2009}, prothesis, or anaptyxis. Indeed, \citet{Davidson2006b} finds that English speakers use all three of these repairs when asked to mimic foreign pronunciations of obstruent-nasal onsets impossible in English.\footnote{However, \citet{Davidson2005,Davidson2006a} finds acoustic and articulatory distinctions between the ``exscrescent'' schwa used to resolve non-native onset clusters, and the ``lexical'' schwa found in real words. For example, the anaptyctic [z${^\textrm{ə}}$k] produced by English speakers attempting to mimic word-initial [zk] more closely resembles the onset of \emph{scum} than the start of \emph{succumb}. This may undermine any link between non-native realizations and epenthesis of schwa in native words (e.g., in the syllabic allomorphs of the regular past tense and noun plural).}

However, prosodic well-formedness distinctions do not provide a ready explanation for the ``numerous other distinctions'' posited in \emph{SPE}. This is thought to rule out simple prosodic account of wordlikeness judgements.

\begin{quote}
A defect of current grammatical acounts of phonotactics is that they render simple up-or-down decisions concerning well-formedness and cannot account for gradient judgements. \citep[371]{Shademan2006}
\end{quote}

Implicit in this claim is the assumption that the graded judgements obtained from wordlikeness tasks which permit gradient responses require a gradient model of well-formedness. However, it is argued here this is not pertinent to the question of whether wordlikeness is gradient or categorical, because graded judgements are a property of all graded tasks, even when applied to judgements of discrete categories.

\citet[382]{Hayes2008a} ``consider the ability to model gradient intuitions to be an important criterion for evaluation phonotactic models''. However, they fail to ask consider whether categorical models provide a reasonable fit to gradient judgements. Rather than directly evaluating the categorical model of English phonotactics proposed by \citet{Clements1983}, \citeauthor{Hayes2008a} convert these constraints, many of which are exceptionless, into probabilitistic constraints. This suggests that \citeauthor{Hayes2008a} equate ``the ability to model gradient intuitions'' with the ability to make continuous predictions about well-formedness. But if gradient judgements are nothing more than a task effect, a model need not make gradient predictions to fit gradient judgements.

Despite many claims that categorical models are empirically insufficient, there is not a single quantitative investigation of categorical wordlikeness models, and no study which compares categorical and gradient wordlikeness models on an equal footing. This chapter represents a first attempt to fill that lacuna. 

OUTLINE HERE

\section{What wordlikeness might not be}

\subsection{Gradience in wordlikeness judgements}

\subsection{Gradience as a task effect}

\section{Evaluation} 
\label{2evaluation}

\subsection{Method}

\subsubsection{Materials and subjects}

\subsubsection{Computational models}

\subsection{Results}

\subsection{Discussion}

\section{Conclusions}

%\citet{Armstrong1983} cut the Gordian knot by asserting the possibility of observing a false consequent; they find the existence of all-or-nothing concepts like ``odd number'' or (more controversially) ``female'' to be apparent.
%
%\begin{quote}
%Are there definitional concepts? Of course. For example, consider the concept \emph{odd number}. This seems to have a clear definition, a precise description. [\ldots{}] No integer seems to sit on the fence, undecided as to whether it is quite even, or perhaps a bit odd. No odd number seems odder than any other odd number. \citep[274]{Armstrong1983}
%\end{quote}
%
%This proposition has the form of an indicative conditional. While the antecedent of this condition, the presence or absence of intermediate ratings in a graded judgement task, is readily observable, it has no bearing on the truth or falsity of the conditional proposition. However, whereas a false consequent (e.g., an all-or-nothing concept) would potentially falsify the proposition, the consequent, a mental state, cannot be directly observed. If this is the case, then both the proposition and the consequent (the proposition that wordlikeness is itself gradient) fail the test of falsification, placing it beyond the scope of scientific inquiry.
%
%\subsection{Gradience as a liability}
%
%One might reasonably ask whether 
%representations 
%
%
%\citeauthor{Armstrong1983} note that if subjects produce intermediate ratings for these categories, then the naïve proposition is false, and this is what \citeauthor{Armstrong1983} find in their experiments. They ask subjects to rate, on a seven-point scale, the extent to which, e.g., certain odd counting numbers represent the concept ``odd number'', and find that speakers do make use of intermediate values, as shown in Figure \ref{agg}. In summary, the proposition that intermediate ratings indicate mental gradience is either untestable, or it is false, and is thus rejected.
%
%\begin{figure} 
%\centering
%\includegraphics{agg.pdf}
%\caption{Subjects asked to rate the degree to which certain even and odd numbers represent the categories ``even'' and ``odd'', respectively, freely use intermediate ratings.}
%\label{agg} 
%\end{figure}
%
%\citeauthor{Armstrong1983} do not view their results as evidence that subjects have an internal model of odd numbers which is at odds with the formal, all-or-nothing definition, and in fact, several additional experiments show that subjects deploy the extension of the formal definition in other tasks. Reviewing this study, \citet[215]{Schutze2011} writes that the experiments show that judgements are ``sensitive to factors other than our underlying competence''. In the case of wordlikeness, a case can be made that one such factor is similarity to existing words (e.g., \citealt{Bailey2001}, \citealt[][151, fn. 27]{LSLT}, \citealt{Greenberg1964}, \citealt{Ohala1986b}, \citealt{Schutze2005}, \citealt{Sendlmeier1987}, \citealt{Vitz1973}). However, the mere fact that subjects use intermediate ratings does not show that they do so with any systematicity, or that, e.g., the contrast between \emph{447} and \emph{7} with respect could be given any satisfying explanation. \citeauthor{Schutze2011} further suggests that gradient responses are the product of the task itself:
%
%\begin{quote}
%Putting it another way, when asked for gradient responses, participants will find some way to oblige the experimenter; if doing so is incompatible with the experimenter's actual question, they apparently infer that she must have really intended to ask something slightly different. \citep[215]{Schutze2011}
%\end{quote}
%
%This introduces a troubling possibility, that subjects ``oblige'' experimenters by introducing random noise to their categorical judgements when presented with a Likert task. Were this the case, the observed intermediate judgements are unlikely to yield information about speakers' knowledge of possible and impossible words. The alternative considered here is that there exists some model in which the gradience in ratings is systematic. 
%
%The evaluation makes exclusive use of previously published English wordlikeness data. For inclusion in the evaluation, a study must conform to all three of the following conditions. First, the stimuli must consist only of monosyllabic words presented auditorily. Secondly, a significant portion of the stimuli must contain gross phonotactic violations (e.g., \emph{bnick}). Finally, the ratings, averaged across subjects, must be publicly available. The large-scale studies by \citet{Bailey2001} and \citet{Shademan2006,Shademan2007} are ineligible because the former lack stimuli with gross phonotactic violations and the latter data are not available to the public in any form. All data are reproduced in Appendix \ref{appendixA}.
%
%\subsubsection{\citealt{Scholes1966}}
%
%\citet{Scholes1966} conducted a number of English wordlikeness judgement with middle school children. The data used here come from his ``experiment 5'', in which 63 monosyllabic items were presented to 33 seventh-grade students. For each stimulus, the subjects produced forced choice ``yes''/``no'' answers to the question of whether the item ``is likely to be usable as a word of English''. \citet{Hayes2008a} and \citet{Albright2009a} analyze this data as gradient by performing an item averaging using the fraction of ``yes'' answers for each stimulus \citep[see also][]{Pierrehumbert1994,Coleman1997,Frisch2000}. For instance, 22 of the 33 students answered ``yes'' for \emph{shlerk} [ʃlɚk], so it is assigned a score of $0.666$.\footnote{This procedure conflates intraspeaker variation, which in some cases is considerable \citep{Shademan2007} with speaker-internal gradience, and its adoption here should not be construed as an endorsement. Rather, it is provided solely for comparison with prior studies using the \citeauthor{Scholes1966} data.} These stimuli are all ostensibly non-words, but include \emph{clung} [klʌŋ], the preterite and past participle of the verb \emph{cling}, and \emph{brung} [brʌŋ], a dialectical past participle for \emph{bring}. These two words were excluded and the remaining 61 stimuli were submitted to analysis. 
%
%\subsubsection{\citealt{Albright2003b}}
%
%\citet{Albright2003b} gathered wordlikeness judgements to serve as norms for a \emph{wug}-test. 87 items were presented to 24 undergraduate subjects, who rated each word on a seven-point Likert scale. The lowest point on the scale was labeled ``completely bizarre, impossible as an English word'', and that the highest point was labeled ``completely normal, would make a fine English word''.
%
%\begin{table}
%\centering
%\begin{tabular}{l r r r}
%\toprule
%                        & subjects & items & trials \\
%\midrule
%\citealt{Albright2003a} & 24       & 87    & 2,088  \\
%\citealt{Albright2007}  & 68       & 40    & 2,720  \\
%\citealt{HayesInPress}  & 29       & 80    & 2,320  \\
%\citealt{Scholes1966}   & 33       & 63    & 2,178  \\
%\midrule
%\textsc{Total}          & 154      & 270   & 9,306  \\
%\bottomrule
%\end{tabular}
%\caption{Subject and item counts}
%\label{counts}
%\end{table}
%
%\subsection{Model comparison}
%
%The four models used here consist of a binary baseline and three computationally implemented gradient models. These three models are chosen because prior studies have shown they are correlated with wordlikeness judgements. Non-parametric rank correlation statistics are used evaluate the correlation between model predictions and wordlikeness ratings. Rank correlation are used rather than the parametric Pearson correlation statistic that is used by \citet{Hayes2008a}, for instance, because they make none of the potentially troublesome assumptions of the latter method \citep[see][23, fn. 12]{Albright2009a}.
%
%The Spearman $\rho$ is most widely known rank correlation statistic, but it is difficult to give a natural interpretation to this quantity. On the other hand, the Kendall $\tau_b$ and Goodman-Kruskal $\gamma$ can be interpreted as fractions of the number of \emph{concordant} and \emph{discordant} pairs \citep{Noether1981}. Consider the case here, in which model scores are compared with wordlikeness ratings. If a model rates \emph{dresp} [dɹɛsp] more wordlike than *\emph{srest} [sɹɛst] and if speakers further rate \emph{dresp} more English-like than \emph{srest}, than \emph{dresp}/\emph{srest} are a concordant pair. If, however, a model and speakers disagree on the relative ranking of \emph{dresp} and \emph{srest}, the pair is discordant. The $\tau_b$ and $\gamma$ differ only in their treatment of ``ties'' (i.e., if e.g., *\emph{dresp} and *\emph{srest} are scored the same, or rated the same); $\tau_b$ includes a correction for ties, whereas $\gamma$ ignores tied pairs. Much like the familiar Pearson correlation, $\rho$, $\tau_b$ and $\gamma$ are all in the range [$-1$, $1$]. Correlations of the four models are given in Table \ref{cor}.
%
%\subsubsection{Binary baseline}
%
%The binary baseline used here is a crude implementation of the null hypothesis that there are no gradient effects in wordlikeness judgements. To create such a baseline, it is necessary to distinguish between nonce words which contain a gross phonotactic violation and those which do not. As all stimuli here are monosyllables, this task can be further simplified by separately considering the two major subcomponents of the syllable, the onset and rime. Speakers are particularly adept as separating onsets from rimes \citep{Treiman1986,Treiman1995,Fowler1993}, and a large portion of phonotactic generalizations and violations can be localized to either onset or rime \citep[e.g.,][]{Borowsky1989,Fudge1969,Treiman1995,Kessler1997,Treiman2000}.
%
%The baseline considers a nonce syllable to be well-formed if it consists of both an attested onset and an attested rime; the free combination of these two components is the only mechanism by which this model can generalize beyond attested words. It is surely the case that this is an ultimately insufficient model: \citet{Albright2009a} observes, for instance, that [ɛsp] is a well-formed rime, even though it is found in no word of English. This problem is more acute in languages with more permissive syllable structures than those of English. For instance, \citet{Fischer-Jorgensen1952} and \citet{Vogt1954} assert that there are many accidental gaps (i.e., possible but unattested structures) in the inventory of consonant clusters in Georgian, a language which admits as many as five adjacent consonants.\footnote{
%Chapter \ref{clusters} 
%%\citet{Gorman2012c}
%reports a similar result for English syllable contact clusters.} Despite these defects, the baseline outperforms the gradient models in most contexts.
%
%The rating densities from the three studies, linearly transformed to fit within the interval [0, 1] and split according to this binary baseline, are shown in Figure \ref{dsn}. In all three studies, it is possible to discern a relatively sharp separation between valid and invalid clusters, but also the presence of intermediate values.
%
%\begin{figure} 
%\centering
%\includegraphics{density.pdf}
%\caption{Average ratings of individual nonce words, linearly transformed to fit the interval [0, 1], tend to clump into two groups with little overlap; words which consist of  attested onsets and rimes receive ratings near ceiling, whereas ratings of phonotactically invalid words are spread across the lower half of the spectrum.}
%\label{dsn}
%\end{figure}
%
%\begin{table} 
%\centering
%\begin{tabular}{l l l l l}
%\toprule
%Spearman $\rho$          & baseline         & maxent  & bigram           & density          \\
%\midrule
%\citealt{Greenberg1964}  & {0.845}          & {0.765} & {\textbf{0.863}} & {0.648}          \\
%\citealt{Scholes1966}    & {0.791}          & {0.762} & {\textbf{0.827}} & {\textbf{0.827}} \\
%\citealt{Albright2003b}  & {0.725}          & {0.429} & {0.708}          & {\textbf{0.742}} \\
%\midrule
%Kendall $\tau_b$         & baseline         & maxent  & bigram           & density          \\
%\midrule
%\citealt{Greenberg1964}  & {\textbf{0.716}} & {0.585} & {0.692}          & {0.462}          \\
%\citealt{Scholes1966}    & {\textbf{0.664}} & {0.597} & {0.652}          & {0.565}          \\
%\citealt{Albright2003b}  & {\textbf{0.599}} & {0.343} & {0.506}          & {0.556}          \\
%\midrule
%Goodman-Kruskal $\gamma$ & baseline         & maxent  & bigram           & density          \\
%\midrule
%\citealt{Greenberg1964}  & {\textbf{1.000}} & {0.684} & {0.692}          & {0.462}          \\
%\citealt{Scholes1966}    & {\textbf{0.995}} & {0.634} & {0.667}          & {0.614}          \\
%\citealt{Albright2003b}  & {\textbf{0.953}} & {0.656} & {0.509}          & {0.575}          \\
%\bottomrule
%\end{tabular}
%\caption{Rank correlations between wordlikeness ratings and phonotactic models surprisingly reveal that the binary baseline meeds or exceeds the coverage of three state-of-the-art phonotactic models. All correlations are significant at $p = 0.05$.}
%\label{cor}
%\end{table}
%
%
%\subsubsection{Maximum entropy phonotactics}
%
%\citeauthor{Hayes2008a} (\citeyear{Hayes2008a}; henceforth H\&W) develop a sophisticated model of phonotactic grammaticality which estimates a probability distribution over phoneme sequences by weighing constraints according to the principle of maximum entropy, following \citet{Goldwater2003} and \citet{Jager2007}. H\&W report that the predictions of their model are closely correlated with the \citet{Scholes1966} wordlikeness ratings. A direct replication of their predictions was attempted by using the software, model parameters, and training data as described in that study. Since the training of the maximum entropy model is inherently stochastic, producing slightly different outcomes on each run, the lowest scoring of ten runs is reported (H\&W:396), though in general there is not a great deal of variation between individual runs. One limitation of this model is that it is not feasible to score whole words, as the number of constraints which must be inspected grows exponentially as the span of possible constraints increases. Following H\&W and of \citet{Albright2009a}, who also applies the maximum entropy model to the \citet{Albright2003b} norms, the model is trained and scored only on stimulus onsets. However, as a consequence, the maximum entropy model performs particularly poorly on this data set, as many stimuli contain phonotactic violations in rime positions.
%
%\subsubsection{Segment bigram probability} 
%\label{bigram}
%
%The bigram probability of a sequence $ijk$ is the product of the probability of an sequence-initial $i$, the probability that $j$ follows $i$, and the probability that $k$ follows $j$, and the product of sequence-final \emph{k}.
%
%\begin{unlabeledexample}
%$\displaystyle \hat{p}(ijk) = p(i|\textrm{start}) \cdot p(j|i) \cdot p(k|j) \cdot p(\textrm{stop}|k)$
%\end{unlabeledexample}
%
%\noindent \citet{Albright2009a} employs bigram probability to model wordlikeness judgements. While the focus of \citeauthor{Albright2009a}'s study is on developing a model which uses bigrams over phonological features rather than segments themselves, \citeauthor{Albright2009a}'s evaluation, which includes both the \citeauthor{Scholes1966} and \citeauthor{Albright2003b} data sets, finds an advantage for segmental bigrams. \citeauthor{Albright2009a} does not provide an implementation of the featural bigram model, nor does his study describe it in sufficient detail to allow for a new implementation, but segmental bigram scores for the \citeauthor{Albright2003a} data are reported in the appendix. As reported by \citeauthor{Albright2009a}, segmental bigrams outperform featural bigrams (see Table \ref{albrightimproved}).
%
%\citeauthor{Albright2009a} estimates bigram probabilities using the method of maximum likelihood over types in the lexicon. The variant of segmental bigrams used here computes probabilities with a simple type of smoothing in which the count of all possible bigrams (including those never observed) are incremented by one. This technique is known as Laplace, or ``add one'' smoothing. This has the desirable effect that no nonce word is ever assigned a zero probability, and produces a small increase in the correlation between the \citeauthor{Albright2003b} wordlikeness norms compared with the maximum likelihood estimate (Table \ref{albrightimproved}). For all three data sets, this model also consistenly outperforms positional probability models defined by \citet{Vitevitch2004} and \citet{Vaden2009}; given that these model scores are highly correlatd with bigram probability \citep[][54]{Vitevitch1997}, they are not considered further. The bigram model consistently performs well in all the evaluations, and has the highest Spearman correlation with the \citeauthor{Greenberg1964} and \citeauthor{Scholes1966} data, and is frequently second place model to the binary baseline elsewhere.
%
%\begin{table} 
%\centering
%\begin{tabular}{l r r r r}
%\toprule
%                         & \multicolumn{1}{c}{featural bigrams} & \multicolumn{2}{c}{segmental bigrams}  \\
%                         & maximum likelihood                   & maximum likelihood & Laplace smoothing \\
%\midrule
%%Pearson $\rho
%Spearman $\rho$          & 0.638                                & 0.660              & \textbf{0.708}    \\
%Kendall $\tau_b$         & 0.457                                & 0.467              & \textbf{0.506}    \\
%Goodman-Kruskal $\gamma$ & 0.462                                & 0.473              & \textbf{0.509}    \\
%\bottomrule
%\end{tabular}
%\caption{Segmental bigrams outperform ``featural bigrams'', and Laplace smoothing increases the correlation between the segmental bigram model proposed by \citet{Albright2009a}, which uses maximum likelihood estimation, and the \citet{Albright2003b} wordlikeness norms. All correlations are significant at $p = 0.05$.}
%\label{albrightimproved}
%\end{table}
%
%\subsubsection{Neighborhood density} 
%\label{density}
%
%There are now many methods for computing similarity between nonce words and existing words, long thought to be reflected in wordlikeness judgements. 
%For this study, a number of such methods were evaluated, including the Generalized Neighborhood Model \citep{Bailey2001}, PLD20 \citep{Suarez2011}, and a number of variations on neighborhood density \citep{Coltheart1977} provided by \citet{Vaden2009}. The best performance was obtained with the simplest version of neighborhood density, which is defined as the number of real monomorphemic words which can be changed into the target nonce word by a single insertion, deletion, or substitution of a phone.\footnote{\citet{Greenberg1964} use a variant in which only substitutions are counted.} For instance, the neighbors of \emph{blick} include \emph{blink} (insertion), \emph{lick} (deletion) and \emph{black} (substitution). While many studies \citep[e.g.,][]{Bailey2001} report robust lexical similarity effects, it may be that the relatively weak performance of neighborhood density is the result of the presence of gross phonotactic violations.
%
%\subsection{Modeling residual gradience}
%
%The primary result is that no gradient model reliably exceeds the accuracy of the binary baseline. Despite this, there are relatively strong correlations between the binary baseline and these gradient models (see Table \ref{bcor}). From the strong performance of the categorical model one can infer that the gradient models do not reliably predict intermediate ratings, or contrasts in ratings between words which are grouped together. To quantify this, the following method was used to estimate the residual contribution of the three gradient models once gross phonotactic violations are taken into account. Instead of calculating rank correlations directly on the model scores as in Table \ref{cor}, the model scores are mapped to ranks with the additional constraint that all ``valid'' stimuli be ranked above all ``invalid'' stimuli. The resulting ranks are used to compute new correlation statistics. Finally, the binary baseline correlation is subtracted from this number, so that the resulting value is the amount of improvement derived from augmenting the binary model with gradience. These difference numbers are shown in Table \ref{controlled}. In most cases, including the gradient models on top of the binary baseline produces a worse correlation than is obtained with the binary baseline alone.
%
%For $\tau_b$ and $\gamma$, the interpretation of this result is clear. The gradient models assign rankings to the sets of phonotactically valid and invalid clusters, respectively. For instance, the bigram model favors \emph{troog} [tɹuːɡ] over \emph{swach} [swætʃ], though neither contains any gross phonotactic violation. Similarly, the bigram model favors \emph{chwoop} [tʃwuːp] over \emph{zhrick} [ʒɹɪk], even though both contain ill-formed onsets. However, the majority of such predicted contrasts are not reflected in speakers' judgements; for instance, \emph{troog} is rated less English-like than \emph{swach} \citep{Greenberg1964}, contrary to the model predictions. This shows quite starkly that these models fail to reliably predict intermediate ratings.
%
%\begin{table} 
%\centering
%\begin{tabular}{l r r r}
%\toprule
%Kendall $\tau_b$          & maxent         & bigram         & density  \\
%\midrule
%\citealt{Greenberg1964}   & 0.670          & \textbf{0.680} & 0.501 \\
%\citealt{Scholes1966}     & \textbf{0.685} & 0.632          & 0.639 \\
%\citealt{Albright2003b}   & 0.542          & 0.603          & \textbf{0.623} \\
%\bottomrule
%\end{tabular}
%\caption{The binary baseline is strongly correlated with the three gradient model scores; all correlations are significant at $p = 0.05$.}
%\label{bcor}
%\end{table}
%
%\begin{table} 
%\centering
%\begin{tabular}{l r r r}
%\toprule
%$\Delta$ Spearman $\rho$          & maxent            & bigram            & density  \\
%\midrule
%\citealt{Greenberg1964}  & $-0.060$          &  \textbf{0.038} & $-0.017$ \\
%\citealt{Scholes1966}    & $-0.029$          &  \textbf{0.047} & $-0.035$ \\
%\citealt{Albright2003b}  & $-0.008$          & $-0.015$          & \textbf{0.018} \\
%\midrule
%$\Delta$ Kendall $\tau_b$         & maxent            & bigram            & density  \\
%\midrule
%\citealt{Greenberg1964}  & $-0.114$          & \textbf{$-$0.007} & $-0.084$ \\
%\citealt{Scholes1966}    & $-0.067$          & \textbf{0.003}  & $-0.061$ \\
%\citealt{Albright2003b}  & \textbf{$-$0.038} & $-0.092$          & $-0.049$ \\
%\midrule
%$\Delta$ Goodman-Kruskal $\gamma$ & maxent            & bigram            & density  \\
%\midrule
%\citealt{Greenberg1964}  & $-0.268$          & \textbf{$-$0.260} & $-0.337$ \\
%\citealt{Scholes1966}    & $-0.361$          & \textbf{$-$0.313} & $-0.345$ \\
%\citealt{Albright2003b}  & \textbf{$-$0.137} & $-0.443$          & $-0.386$ \\
%\bottomrule
%\end{tabular}
%\caption{The change in rank correlation generated by augmenting the purely binary model with gradient predictions is small and in most cases it is negative.}
%\label{controlled}
%\end{table}
%
%\subsection{The gradience hypothesis}
%
%This chapter has evaluated the axiom of gradience as a falsifiable alternative hypothesis. The surprising result is that virtually all of the apparent coverage of state-of-the-art gradient phonotactic models is simply a reflection of their ability to distinguish between the possible and the totally impossible; beyond this, they are unreliable. A trivial baseline, endowed with few abilities to project beyond the observed data, generally outperforms the state of the art. The projections made by the state-of-the-art gradient models are not like those made by speakers. It remains to be seen is whether any model can be put forth which accurately predicts these intermediate ratings.
%
%These result provide support for recent findings that speakers asked to perform gradient syntactic judgements produce responses closely corresponding to a widely recognized categorical grammatical/ungrammatical distinction \citep{Sprouse2007}.
%
%\subsection{Extensions to the binary baseline}
%
%The strong performance of the binary baseline should not be taken as evidence either that wordlikenesss judgements are binary, or that the binary baseline is a plausible model. The most serious limitation of this evaluation is the primitive nature of the binary baseline. The inability to generalization within onsets and rimes is a serious flaw, as is the assumption of independence of onset and rime. Regarding the rime, \citet{Borowsky1989} proposes a theory of possible rimes in English, which does make the correct prediction regarding the unattested but well-formed [ɛsp]. On the other hand, a cognitively plausible version of this model might need to entertain phonotactic generalizations that are larger than these units, since syllable-sized phonotactic generalizations have been proposed for English \citep[e.g.,][]{Berkley1994a,Berkley1994b,Coetzee2008b,Fudge1969}. 
%
%A possible further extension to the binary baseline would be the introduction of additional levels of wellformedness. While the evaluation has shown that current gradient models do not reliably identify intermediate wellformedness, it does seem possible to identify at least three levels of grammaticality: for instance, one might encode the intuition that \emph{zhlick} [ʒlɪk] is more English-like than \emph{bnick}, though both have unattested onsets. There are precedents for labeling certain attested words as phonotactically ``peripheral'' (see, e.g., the appendices in \citealt{Myers1987} and \citealt{Borowsky1989}); such words are regarded as lexical exceptions to language-general principles of syllabification. If this extends to nonce words, then an intermediate level of grammaticality could be assigned to ``possible'' but formally marked words. Another likely source of additional levels of grammaticality is the cumulative effect of multiple phonotactic violations. While, as \citet{Coleman1997} note, classical Optimality Theory predicts that a nonce word is as ill-formed as its worst deviation from syllable structure, it is possible to imagine that multiple phonotactic violations would result in greater degrees of ill-formedness. The bigram and maxent models make this prediction, as do many others \citep[e.g.,][]{Legendre1990,Levelt2000,Albright2008,Anttila2008a,Pater2009b} but despite this, there is still little data demonstrating cumulative effects in wordlikeness tasks.
%
%\subsection{Language acquisition}
%
%The weak empirical status of gradient phonotactic knowledge as reflected in adults has rammifications for language acquisition under the hypothesis that that infants acquiring language deploy the same representations as adults \citep[e.g.,][]{Macnamara1982,Pinker1984,Crain1991,Carey1995,deVilliers2001,Legate2007}. Gradient wordlikeness judgements in adults would provide support for claims that infants recognize statistical(inherently gradient) dependencies between segments \citep{Jusczyk1994} and use these to segment words \citep{Saffran1996}. An emerging consensus suggests, however, that infants attend to transitional probabilities primarily in the absence of grammatical cues \citep{Gambell2005,Hohne1994,Johnson2001,Jusczyk1999c,Lignos2012b,Mattys2001a,Shukla2007,Lew-Williams2012}. The vacuous nature of current evidence for gradient phonotactic knowledge in adults further weakens any hypothesis that would link statistical learning in infants to adults' behaviors.
%
%\section{Conclusion}
%
%%For instance, infants as young as 4.5 months seem to be aware that English nasal codas agree in place with following obstruents \citep{Mattys2001b,Jusczyk2002,Davidson2004}. 
%%It might be the case that syllable co-occurrence statistics might be little more than a reflection of infants' learning of categorical ``lexical viability'' constraints \citep{Johnson2003} of the sort also seen in adult speech processing \citep[e.g.,][]{Norris1997}. 
%
%% other ratings, but por qué?:
%% 
%% Hayes2000
%% 
%% Albright2003a
%% Albright2003b
%% Prasada1993 
%% 
%% Bard1996
%% 
%% Koo2009
%% 
%% Treiman2000
%% 
%% Warker2006
%% 
%% Massaro1983
%% 
%% Rusaw2009
%do not explicitly state why this data is relevant to the construction of models of wordlikeness. Presumably, these authors believe that these patterns of judgements demonstrate that wordlikeness, as an internal state, is gradient simply because subjects make use of intermediate degrees of wordlikeness in judgement tasks. This proposition, generalized below, is ``naïve'' not because it lacks sophistication, but because it is rooted in a belief in naïve realism, a philosophy which holds that perception provides a relatively direct picture of the nature of the world, an influential view in the cognitive sciences in general (see \citealt{Fodor1981a} for a critique).
%
%\citeauthor{Chomsky1965} were not the first to consider the notion of possible and impossible words. Their primary contribution is that their mentalist perspective: they recognize that naïve speakers effortlessly acquire language-specific generalizations about possible and impossible words and can report them without any explicit training.
%
%However, not all early literature is concerned with gradience. \citet[31]{Vogt1954}, for instance, recognizes that the taxonomic phoneme is insufficient to account for many wordlikeness contrasts. \citeauthor{Vogt1954} observes that allophony may account for the absence of certain phone sequences, but it does not provide a suitable explanation for the absence of initial [bn] in English, nor does it make correct predictions about the surface realization of an underlying initial /bn/. \citeauthor{Vogt1954} concludes that additional grammatical machinery will be needed to account for possible and impossible words. 
%
%Most relevant to the question at hand, \citet{Frisch2000} and \citet{Vitevitch1997} find that speakers' wordlikeness ratings of multisyllabic words are correlated wtih the positional probailities of the constituent syllables. Unfortunately, none of these researchers make any effort to eliminate the possibility that the low positional probability stimuli are ``impossible'' words of English. In fact, 
%Chapter \ref{clusters} argues
%%the author has argued elsewhere \citep{Gorman2012c}
%that many of the stimuli used by \citeauthor{Frisch2000} and \citeauthor{Vitevitch1997} contain illicit word-medial consonant clusters. While \citeauthor{Vitevitch1997} neither control nor manipulate the well-formedness of medial clusters, in a post-hoc test they consider a probabilistic measure of cluster well-formedness, which reveals that cluster well-formedness is correlated with syllable-internal positional probabilities and wordlikeness judgements, but \citeauthor{Vitevitch1997} ultimately conclude this cannot explain all the variation in wordlikeness. 
%
%Using the head-term preference paradigm, \citet{Jusczyk1993b} and \citet{Friederici1993} find that typically-developing children as young as 9 months of age distinguish between nonce words which are and are not phonotactically valid in their target language. \citet{Jusczyk1994} report that 9-month-old children acquiring English also show preferences for nonce words with high positional probability over those with low positional probability. Faciliatory effects of positional probability (i.e., shorter latencies) are reported for other nonce word tasks conducted with adults, including single-word shadowing \citep{Vitevitch1997,Vitevitch1998}, same/different judgements \citep{Vitevitch1999a,Luce2001,Lipinski2005,Vitevitch2005}, and lexical decision \citep{Pylkkanen2002a}.
%
%The aforementioned studies all conclude that the gradient measure of positional probability correlates with behavioral results. As the flaws of the \citet{Vitevitch1997} study demonstrate, the aforementioned studies do little to tease apart the gradient and categorical aspects of phonotactics. More generally, they do little to distinguish between positional probability and closely correlated measures like bigram probability (see \S\ref{bigram} below) or neighborhood density, since these studies carefully select stimuli which either have high or low values for all of positional probability, bigram probablility, and neighborhood density. This is particularly troublesome given that no justification has ever been given for the positional probability measure in the first place; it appears to have been created \emph{ex nihilo}; in contrast, the effects of neighborhood density in various psycholinguistic tasks are emergent properties of many models of speech production \citep[e.g.,][]{Luce1998,Luce2000} and perception \citep{Marslen-Wilson1984,Marslen-Wilson1987,McClelland1986,Norris1994,Norris2000}. 
%\subsubsection{\citealt{Greenberg1964}}
%
%\citet{Greenberg1964} investigated wordlikeness using the technique of free magnitude estimation, a mechanism which has become increasingly popular among syntacticians \citep[e.g.,][]{Bard1996}. At the beginning of the experiment, the subject heard a recording of the word \emph{stick}. In subsequent trials, the subjects heard a nonce word and were asked to report ``how far would you say that is from English?'', with \emph{stick} at ``1''; subjects are told that a word that is ``twice as far from English'' as \emph{stick} should be scored ``2''. The data used here are from \citeauthor{Greenberg1964}'s Experiment B, in which 17 undergraduates were presented 17 stimuli in all. In addition to \emph{stick}, the stimuli include three other English words; these four items were excluded from further analyses, leaving 13 stimuli. As is standard practice in psychophysics \citep[e.g.,][]{Butler1987}, magnitudes were log-transformed prior to analysis.
%
