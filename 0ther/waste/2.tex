%\subsection{Future directions}
%\subsubsection{Interspeaker variation}
%The studies analyzed above all use ratings averaged across subjects. While aggregation of this type is quite common in psycholinguistics, it is undesirable as a statistical practice: it drastically increases the chance of a Type II error, the error of failing to reject a null hypothesis when the null hypothesis is false \citep[][8f.]{Baayen2004}. Further, little is known about intraspeaker variation in wordlikeness judgements and even less about intraspeaker variation, but \citet{Shademan2007} does find suggestive differences between older and younger adults. It would be extremely interesting to correlate differences in speakers with external factors like age, level of education, and exposure to foreign languages. 
%\subsubsection{Mixed modalities}
%While the use of auditory stimulus presentation seems preferable to rating orthographic words, the possibiltiy of misperception introduces a potential confound. \citet{Scholes1966} mentions that a few of his stimuli were frequently misperceived, and a number of studies \citep{Massaro1983,Halle1998b,Pitt1998,Dupoux1999,Berent2007a,Davidson2007} uncover positive correlations between phonotactic acceptability and accurate perception. If the phonotactically invalid cluster in *\emph{srest} is perceived as a phonotactically valid cluster with similar acoustic features, such as [ʃr], it will likely receive an inflated wordlikeness rating. In many psycholinguistic tasks, the behavioral measure of interest comes with a validation of correct perception: for instance, in shadowing tasks, reaction times are the primary behavioral measure but shadowing errors are also informative behavioral measures \citep[e.g.,][]{Marslen-Wilson1978}. Wordlikeness studies could combine transcription or repetition tasks with judgement tasks to confirm correct perception and perhaps to enhance the online nature of the task.
%Several recent studies have investigated the role of phonotactics using an auditory lexical decision tasks \citep[e.g.,][]{Berent2001a,Berent2004,Shatzman2007a}. A recent study by \citet{Coetzee2008b} is particularly relevant to the larger questions of this dissertation, though the results are ultimately inconclusive. \citeauthor{Coetzee2008b} claims that words of the form [spVp, skVk] (e.g., *\emph{spep} [spɛp], *\emph{skeek} [skiːk]) are impossible in English, whereas [stVt] words (e.g., *\emph{stoit} [stɔɪt]) are wellformed. In addition to a traditional wordlikeness task, \citet{Coetzee2008b} includes stimuli of this type in a lexical decision task, under the hypothesis that the illformed nonce words, those of the shape [spVp, skVk], will be more quickly rejected than valid [stVt] stimuli, a prediction which is borne out. However, there is no need to provide a phonotactic interpretation of these effects. It is known that auditory tasks reaction times are significantly slower for non-words with dense cohorts, that is, nonce words like *\emph{hup} [hʌp] which share initial segments with many real words (e.g., \emph{hull}, \emph{hut}, etc.). These effects are found across modalities \citep[e.g.,][]{Marslen-Wilson1978,Marslen-Wilson1984} and appear to be particularly attentuated for non-word onsets \citep{Cole1980,Vitevitch2002}. Since [st] onsets are far more frequent than [sp] or [sk] onsets \citep[][395]{Hayes2008a}, this alone could explain the variation in reaction times. Despite this, nonce word lexical decision with controls for lexical cohort effects shows potential as a substitute for wordlikeness judgements in phonotactics research. 
%\subsubsection{Data availability}
%There are still very few publicly available wordlikeness databases, and several authors who were contacted for this project declined to share data. Not only does this make large-scale model comparison difficult, it also means that most wordlikeness studies cannot be replicated except at the conceptual level. Lexical decision researchers now have databases of response times for American \citep{ELP} and British English \citep{BLP}, Dutch \citep{DLP}, French \citep{FLP}, and Malay \citep{MLP}. Given the simplicity of wordlikeness judgements (a sort of linguistic \emph{Drosophilia}), the time for a publicly available ``English Wordlikeness Project'' has come.
%\subsubsection{Non-parametric statistics}
%The surprising results of this evaluation can be traced directly to two methodological improvements which should be more widely adopted. First, model comparison studies must replace parametric statistical models, such as Pearson correlation, with non-parametric equivalents. Given the tendency of ratings to cluster at endpoints, Pearson correlations are inflated \citep[][23, fn. 12]{Albright2009a} and the assumptions of the test are flagrantly violated. A shift towards non-parametric comparison appears to be well underway: \citet{Hayes2008a} also report model correlations using the Spearman $\rho$ and \citet{Albright2009a} focuses on a variant of the Kendall correlation called $tau_c$. %On the other hand, both \citeauthor{Hayes2008a} and \citeauthor{Albright2009a} ultimately use parametric statistics for model comparison.
%\begin{table}
%\centering
%\begin{tabular}
%\toprule
%                        & accuracy & precision & recall & F$_1$ & MCC   \\
%\midrule
%\citealt{Greenberg1964} & 0.846    & 1.000     & 0.800  & 0.889 & 0.693 \\
%\citealt{Scholes1966}   & 0.952    & 0.947     & 0.900  & 0.923 & 0.889 \\
%\citealt{Albright2003b} & 0.920    & 0.938     & 0.953  & 0.946 & 0.797 \\
%\bottomrule
%\end{tabular}
%\end{table}
% EM \citet{EM} 
%\begin{table}
%\centering
%\begin{tabular}{l | r r}
%\toprule
%                        & $\chi^2_{3}$ & $p$-value  \\
%\midrule
%\citealt{Greenberg1964} &  5.190       & 0.158      \\
%\citealt{Scholes1966}   & 48.964       & 1.3\e{-15} \\
%\citealt{Albright2003b} & 16.186       & 0.001      \\
%\bottomrule
%\end{tabular}
%\caption{This is my caption}
%\end{table}
%\footnote{This is conceptually similar to the technique of residualization discussed by \citet{Gorman2010c}.}
%\begin{unlabeledexample}
%$\displaystyle P = s^{1/7.4}$
%\end{unlabeledexample}
%\noindent Even then, \citet[][23, fn. 12]{Albright2009a} 
%An interesting feature of this data set is that includes a diverse set of onsets ranging from frequent to impossible, but only a few common rimes; \citet{Albright2009a} speculates this might have made the onsets more salient to the subjects than tehy would be otherwise.
%The Pearson correlation depends on an interval assumption, which holds that the difference between, for example, ``1. completely bi disagree'' and ``2. agree'' is in some sense identical to that between ``3. neither agree nor disagree'' and ``4. agree''. \citet{Stevens1946} already observes that there is no a priori basis for this assumption, and similar objections can be leveled for other rating methods. Secondly, 
%the Pearson correlation assumes a linear relationship between model scores and ratings; when this assumption is violated, researchers must search for a satisfactory transformation to produce linearity. 
%\end{quote}
%The similarity between sequence structure constraints and neutralizing phonological rules was noted by \citet[][401f.]{Stanley1967}. Later discussions tend to view it as a problem (\citealt{Kisseberth1970b}, \citealt{Kenstowicz1977}). The relationship between surface and underlying constraints is taken up in the next two chapters.
%\citealt{Barton1986}, \citealt{Barton1987}, \citealt{Anderson1988a}, \citealt{Ristad1994}
%\citet{Vitevitch1999b} % summary paper
%\citealt{Kahlen-Halstenbach1990}, \citealt{Inozuka1991})
